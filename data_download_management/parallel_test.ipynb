{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6570214-7c66-4a34-af9c-99e67064bdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting reconstruction for train split ===\n",
      "Loading Arrow dataset from /scratch/szp2fv/ID_AI_Project/DS6050_Ai_Detection/train/real ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72465220611545b1b5b8b49dfd044221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 57600 images using 8 workers...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import os\n",
    "from PIL import Image as PILImage, ImageFile\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Pillow setup\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "PILImage.MAX_IMAGE_PIXELS = None  # disable decompression bomb check, we it ourseleves\n",
    "MAX_PIXELS = 178_956_970\n",
    "\n",
    "# User settings\n",
    "where_you_saved_data = r\"/scratch/szp2fv/ID_AI_Project/DS6050_Ai_Detection\"\n",
    "SCALE_FACTOR = 0.1 # scaling the super large images\n",
    "MAX_WORKERS = 8  # parallelize over 8 cores, breaks with more\n",
    "LOG_INTERVAL_IMAGES = 1000  # print every N images, this isnt working :/\n",
    "\n",
    "# Worker function\n",
    "def process_image(idx, row, out_folder):\n",
    "    try:\n",
    "        img_data = row[\"image\"]\n",
    "        fname = row.get(\"filename\", f\"{idx:06d}.jpg\")\n",
    "        fname = Path(fname).name\n",
    "        ext = Path(fname).suffix.lower() or \".jpg\"\n",
    "        if ext not in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "            ext = \".jpg\"\n",
    "        fname = f\"{Path(fname).stem}{ext}\"\n",
    "        dst_path = out_folder / fname\n",
    "\n",
    "        # Handle PIL image\n",
    "        if hasattr(img_data, \"save\"):\n",
    "            img = img_data\n",
    "        # Handle numpy array\n",
    "        elif isinstance(img_data, np.ndarray):\n",
    "            img = PILImage.fromarray(img_data)\n",
    "        # Handle bytes\n",
    "        elif isinstance(img_data, bytes):\n",
    "            with open(dst_path, \"wb\") as f:\n",
    "                f.write(img_data)\n",
    "            return \"saved\", 0\n",
    "        else:\n",
    "            return \"skipped\", 0\n",
    "\n",
    "        resized = 0\n",
    "        if img.width * img.height > MAX_PIXELS:\n",
    "            new_size = (int(img.width * SCALE_FACTOR), int(img.height * SCALE_FACTOR))\n",
    "            img = img.resize(new_size, PILImage.Resampling.LANCZOS)\n",
    "            resized = 1\n",
    "\n",
    "        if img.mode in (\"P\", \"RGBA\"):\n",
    "            img = img.convert(\"RGB\") if ext != \".png\" else img.convert(\"RGBA\")\n",
    "\n",
    "        img_format = \"PNG\" if ext == \".png\" else \"JPEG\"\n",
    "        save_kwargs = {\"format\": img_format}\n",
    "        if img_format == \"JPEG\":\n",
    "            save_kwargs.update({\"quality\": 95, \"optimize\": True})\n",
    "\n",
    "        img.save(dst_path, **save_kwargs)\n",
    "        return \"saved\", resized\n",
    "    except Exception as e:\n",
    "        return \"skipped\", 0\n",
    "\n",
    "# Main reconstruction\n",
    "def reconstruct_dataset():\n",
    "    hf_local_path = Path(where_you_saved_data)\n",
    "    splits = [\"train\", \"validation\"]\n",
    "    types = [\"real\", \"fake\"]\n",
    "\n",
    "    overall_start = time.time()\n",
    "\n",
    "    for split in splits:\n",
    "        temp_split_folder = hf_local_path / f\"temp_{split}\"\n",
    "        temp_split_folder.mkdir(exist_ok=True, parents=True)\n",
    "        print(f\"\\n=== Starting reconstruction for {split} split ===\", flush=True)\n",
    "\n",
    "        for dtype in types:\n",
    "            folder_start = time.time()\n",
    "            arrow_path = hf_local_path / split / dtype\n",
    "            if not arrow_path.exists():\n",
    "                print(f\"Skipping: {arrow_path} does not exist.\", flush=True)\n",
    "                continue\n",
    "\n",
    "            print(f\"Loading Arrow dataset from {arrow_path} ...\", flush=True)\n",
    "            ds = load_from_disk(str(arrow_path))\n",
    "\n",
    "            out_folder = temp_split_folder / dtype\n",
    "            out_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            total_imgs = len(ds)\n",
    "            print(f\"Processing {total_imgs} images using {MAX_WORKERS} workers...\", flush=True)\n",
    "\n",
    "            saved_count = 0\n",
    "            skipped_count = 0\n",
    "            resized_count = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Submit all images to the executor\n",
    "            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                futures = [executor.submit(process_image, idx, row, out_folder) for idx, row in enumerate(ds)]\n",
    "\n",
    "                for i, f in enumerate(tqdm(as_completed(futures), total=len(futures),\n",
    "                                           desc=f\"{split}/{dtype}\", unit=\"img\")):\n",
    "                    status, resized = f.result()\n",
    "                    if status == \"saved\":\n",
    "                        saved_count += 1\n",
    "                    else:\n",
    "                        skipped_count += 1\n",
    "                    resized_count += resized\n",
    "\n",
    "                    # Print periodic progress\n",
    "                    if (i + 1) % LOG_INTERVAL_IMAGES == 0:\n",
    "                        elapsed = time.time() - start_time\n",
    "                        speed = (i + 1) / elapsed if elapsed > 0 else 0\n",
    "                        remaining = total_imgs - (i + 1)\n",
    "                        eta_minutes = remaining / speed / 60 if speed > 0 else 0\n",
    "                        print(f\"[{split}/{dtype}] Processed {i+1}/{total_imgs} images \"\n",
    "                              f\"(speed: {speed:.2f} img/s, ETA: {eta_minutes:.2f} min, resized: {resized_count}, skipped: {skipped_count})\",\n",
    "                              flush=True)\n",
    "\n",
    "            folder_duration = time.time() - folder_start\n",
    "            print(f\"\\nFinished {split}/{dtype}: saved {saved_count}, resized {resized_count}, skipped {skipped_count}.\", flush=True)\n",
    "            print(f\"Time taken: {folder_duration / 60:.2f} minutes\\n\", flush=True)\n",
    "\n",
    "            # Remove Arrow folder to save space\n",
    "            shutil.rmtree(arrow_path, ignore_errors=True)\n",
    "\n",
    "        # Cleanup split folder\n",
    "        split_folder = hf_local_path / split\n",
    "        if split_folder.exists():\n",
    "            shutil.rmtree(split_folder, ignore_errors=True)\n",
    "        os.rename(temp_split_folder, hf_local_path / split)\n",
    "\n",
    "    overall_duration = time.time() - overall_start\n",
    "    print(f\"\\nDataset reconstruction complete! Total time: {overall_duration / 60:.2f} minutes\", flush=True)\n",
    "\n",
    "# ==============================\n",
    "# Run\n",
    "# ==============================\n",
    "reconstruct_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c6a5e-ba88-479c-b5e0-bd1688e970a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbddb0dc-4646-42b5-b38a-e8fa09110267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "id_ai",
   "language": "python",
   "name": "id_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
